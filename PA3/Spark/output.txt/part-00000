(u'', 39)
(u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,', 1)
(u'and Spark Streaming for stream processing.', 1)
(u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported', 1)
(u'You can find the latest Spark documentation, including a programming', 1)
(u'    ./bin/run-example SparkPi', 1)
(u'# Apache Spark', 1)
(u'can also use an abbreviated class name if the class is in the `examples`', 1)
(u'"yarn" to run on YARN, and "local" to run', 1)
(u'## Example Programs', 1)
(u'## Online Documentation', 1)
(u'<http://spark.apache.org/>', 1)
(u'    MASTER=spark://host:7077 ./bin/run-example SparkPi', 1)
(u'To run one of them, use `./bin/run-example <class> [params]`. For example:', 1)
(u'And run the following command, which should also return 1000:', 1)
(u'Testing first requires [building Spark](#building-spark). Once Spark is built, tests', 1)
(u'## Building Spark', 1)
(u'package. For instance:', 1)
(u'Spark also comes with several sample programs in the `examples` directory.', 1)
(u'Spark is a fast and general cluster computing system for Big Data. It provides', 1)
(u'To build Spark and its example programs, run:', 1)
(u'["Specifying the Hadoop Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1)
(u'## Configuration', 1)
(u'for detailed guidance on building for a particular distribution of Hadoop, including', 1)
(u'## Running Tests', 1)
(u'    >>> sc.parallelize(range(1000)).count()', 1)
(u'The easiest way to start using Spark is through the Scala shell:', 1)
(u'    ./bin/pyspark', 1)
(u'## Interactive Scala Shell', 1)
(u'    ./bin/spark-shell', 1)
(u'More detailed documentation is available from the project site, at', 1)
(u'[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).', 1)
(u'MLlib for machine learning, GraphX for graph processing,', 1)
